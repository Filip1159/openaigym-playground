# ğŸ§  Notatka z nauki: Reinforcement Learning (RL) â€” podstawy

## ğŸ“¦ Åšrodowisko (*Environment*)

- To **Å›wiat**, w ktÃ³rym dziaÅ‚a agent.
- Definiuje:
  - **przestrzeÅ„ stanÃ³w** (`observation space`) â€” np. pozycja i prÄ™dkoÅ›Ä‡ w CartPole
  - **przestrzeÅ„ akcji** (`action space`) â€” np. ruch w lewo/prawo
  - **nagrody** za wykonane akcje
- W `OpenAI Gym` tworzy siÄ™ Å›rodowisko np. tak:
  ```python
  env = gym.make("CartPole-v1", render_mode=None)
  ```

## ğŸ‘¤ Agent

- **Uczy siÄ™** na podstawie obserwacji i nagrÃ³d, aby maksymalizowaÄ‡ sumÄ™ nagrÃ³d w czasie.
- W kaÅ¼dej chwili wybiera akcjÄ™ na podstawie aktualnego stanu.
- W RL agent czÄ™sto reprezentowany jest jako:
  - Funkcja decyzyjna (np. `policy`)
  - Model (np. sieÄ‡ neuronowa, ktÃ³ra aproksymuje wartoÅ›Ä‡ akcji)

## ğŸ­ Aktor i Krytyk (*Actor & Critic*)

- **Actor** â€“ komponent, ktÃ³ry **podejmuje decyzje** (akcje), czÄ™sto reprezentowany jako `Ï€(s)`
- **Critic** â€“ komponent, ktÃ³ry **ocenia jakoÅ›Ä‡ dziaÅ‚ania aktora**, np. wyznaczajÄ…c wartoÅ›Ä‡ stanu `V(s)` lub wartoÅ›Ä‡ akcji `Q(s, a)`
- W `DQN` (Deep Q-Network) krytyk to sieÄ‡ estymujÄ…ca Q-funkcjÄ™ (patrz niÅ¼ej), a aktorem jest algorytm oparty na `argmax(Q(s, a))`.

## ğŸŸ° Q-funkcja (`Q(s, a)`)

- Szacuje **oczekiwanÄ… sumÄ™ nagrÃ³d**, jakÄ… agent moÅ¼e uzyskaÄ‡, wykonujÄ…c akcjÄ™ `a` w stanie `s` i postÄ™pujÄ…c dalej optymalnie.
- UÅ¼ywana np. w `Q-learningu` i `DQN`.

FormuÅ‚a aktualizacji (tablicowy Q-learning):

```
Q(s, a) â† Q(s, a) + Î± [r + Î³ max_a' Q(s', a') - Q(s, a)]
```

## ğŸ”¤ WaÅ¼ne parametry

| Symbol | Nazwa        | Znaczenie |
|--------|--------------|-----------|
| `Î±`    | **Learning rate** (tempo uczenia) | Jak bardzo nowa wiedza nadpisuje starÄ… |
| `Î³`    | **Discount factor** (czynnik dyskonta) | Jak bardzo agent dba o przyszÅ‚e nagrody |
| `Îµ`    | **Epsilon** (w eksploracji) | PrawdopodobieÅ„stwo losowego dziaÅ‚ania (eksploracja) |
| `r`    | **Reward** (nagroda) | Liczba otrzymana od Å›rodowiska |
| `s`, `s'` | Stan obecny / nastÄ™pny | Opis sytuacji w Å›rodowisku |
| `a`, `a'` | Akcja | CzynnoÅ›Ä‡ podjÄ™ta przez agenta |

## â™»ï¸ Epsilon-Greedy (eksploracja vs eksploatacja)

- Strategia dziaÅ‚ania:
  - Z prawdopodobieÅ„stwem `Îµ` agent wybiera **losowÄ… akcjÄ™** (eksploracja)
  - Z prawdopodobieÅ„stwem `1 - Îµ` wybiera **najlepszÄ… znanÄ… akcjÄ™** (eksploatacja)
- Typowy schemat: `Îµ` zmniejsza siÄ™ w czasie (`decay`), aby na poczÄ…tku agent eksperymentowaÅ‚, a potem siÄ™ stabilizowaÅ‚.

## ğŸ§  Deep Q-Learning (DQN)

- UÅ¼ywa sieci neuronowej do aproksymacji funkcji `Q(s, a)`
- W treningu minimalizujemy tzw. **loss** miÄ™dzy `Q(s, a)` a celem:

```python
loss = mse(Q(s, a), r + Î³ * max_a' Q(s', a'))
```

## ğŸ§ª Replay Buffer (bufor doÅ›wiadczeÅ„)

- Przechowuje przeÅ¼yte przez agenta doÅ›wiadczenia `(s, a, r, s', done)`
- Podczas uczenia losujemy **mini-batch** z bufora, co poprawia stabilnoÅ›Ä‡ i efektywnoÅ›Ä‡ nauki (zmniejsza korelacje czasowe)

## ğŸ’¡ Praktyczne wskazÃ³wki

- Nie renderuj Å›rodowiska podczas treningu (`render_mode=None`) â€“ przyspiesza dziaÅ‚anie.
- UÅ¼yj `render_mode="human"` tylko do demo po treningu.
- Pracuj z `device = torch.device("cuda" if torch.cuda.is_available() else "cpu")`, by Å‚atwo przeÅ‚Ä…czaÄ‡ CPU â†” GPU.
